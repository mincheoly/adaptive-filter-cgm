{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read cleaned (in Excel) CSV data files\n",
    "\n",
    "# order of the columns: Patient ID, Time, Glucose Measurement\n",
    "# remove all columns with NaN\n",
    "\n",
    "gold_data = np.array(pd.read_csv('GoldSample_Data_Cleaned.csv').as_matrix())\n",
    "gold_data = gold_data[~np.isnan(gold_data).any(axis=1)]\n",
    "\n",
    "# order of the columns: Patient ID, Date, Time, Glucose Measurement, Watch # (there are 2)\n",
    "gwb_data = np.array(pd.read_csv('GWB_Data_Cleaned.csv').as_matrix())\n",
    "gwb_data = gwb_data[~np.isnan(gwb_data).any(axis=1)]\n",
    "\n",
    "# CGMS data comes in a dictionary, not a CSV, due to the fact that multiple monitors are present.\n",
    "# We did some hacky things with the raw data to fish out one of the sensors, but it should be fine.\n",
    "# cgm_data is a dictionary already.\n",
    "with open('pt_cgm_longest.p', 'r') as fp:\n",
    "    cgm_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Break up the data into patients\n",
    "# I average datapoints with the same value\n",
    "# Each dictionary is created from a single data source\n",
    "\n",
    "pt_gold_data = {}\n",
    "pt_cgm_data = {}\n",
    "pt_gwb_data = {}\n",
    "for patient_id in np.unique(gold_data[:,0]).astype(int):\n",
    "    patient_indices = (gold_data[:,0]==patient_id)\n",
    "    df = pd.DataFrame(gold_data[patient_indices, 1:], columns = [\"First Col\", \"Second Col\"])\n",
    "    pt_gold_data[patient_id] = df.groupby('First Col', as_index=False).mean().as_matrix()\n",
    "    \n",
    "for patient_id in np.unique(gwb_data[:,0]).astype(int):\n",
    "    patient_indices = (gwb_data[:,0]==patient_id)\n",
    "    df = pd.DataFrame(gwb_data[patient_indices, 1:], columns = [\"First Col\", \"Second Col\", \"Third Col\"])\n",
    "    pt_gwb_data[patient_id] = df.groupby('First Col', as_index=False).mean().as_matrix()\n",
    "        \n",
    "# cgm_data is actually a dictionary, except without the avg processing for same timepoints\n",
    "for patient_id in cgm_data.keys():\n",
    "    data = cgm_data[patient_id]\n",
    "    df = pd.DataFrame(data, columns = [\"First Col\", \"Second Col\"])\n",
    "    pt_cgm_data[patient_id] = df.groupby('First Col', as_index=False).mean().as_matrix()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpolate gold standard data and align with cgm data\n",
    "# This is a dictionary of patients and all of their data (CGM and interpolated gold standard)\n",
    "\n",
    "verbose = False\n",
    "\n",
    "pt_inter_cgm = {} # order of the columns: Time, CGM, BGM\n",
    "\n",
    "for pt_ID in pt_gold_data.keys():\n",
    "    \n",
    "    # get patient cgm and gold data\n",
    "    cgm_data = pt_cgm_data[pt_ID]\n",
    "    gold_data = pt_gold_data[pt_ID]    \n",
    "\n",
    "    # create cubic spline fit for gold_data\n",
    "    cs = interpolate.splrep(gold_data[:,0], gold_data[:,1], s=0, k=1)\n",
    "    \n",
    "    # Interpolate gold standard points for all possible CGM\n",
    "    # Smooth the interpolation with Gaussian filter\n",
    "    inter_gs = np.reshape(interpolate.splev(cgm_data[:,0], cs, der=0),(len(cgm_data),1))\n",
    "    smoothed_inter_gs = gaussian_filter(inter_gs, sigma=3)\n",
    "    temp = np.hstack([cgm_data, smoothed_inter_gs])\n",
    "    \n",
    "    # Get rid of meaningless interpolations, outside the window of overlap for original data\n",
    "    time_lower_limit = np.max([np.min(gold_data[:,0]), np.min(cgm_data[:,0])])\n",
    "    time_upper_limit = np.min([np.max(gold_data[:,0]), np.max(cgm_data[:,0])])\n",
    "    limit = [time_lower_limit, time_upper_limit]\n",
    "    \n",
    "    # Select the valid time points within the overlap window\n",
    "    valid_indices = [idx[0] for idx,timepoint in np.ndenumerate(temp[:,0]) if (timepoint >= limit[0] and timepoint <= limit[1])]\n",
    "    pt_inter_cgm[pt_ID] = temp[valid_indices,:]\n",
    "    \n",
    "    # Visualize for debugging purposes\n",
    "    if verbose:\n",
    "        plt.scatter(gold_data[:,0], gold_data[:,1], color='gold')\n",
    "        plt.plot(pt_inter_cgm[pt_ID][:,0], pt_inter_cgm[pt_ID][:,2], color='gold') # show interpolated gold standard\n",
    "        plt.plot(pt_inter_cgm[pt_ID][:,0], pt_inter_cgm[pt_ID][:,1], color='blue') # show CGM\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save patient data as json\n",
    "\n",
    "# Save the dictionary with interpolated true points and the CGM\n",
    "with open('pt_cgm_bgm.p', 'wb') as fp:\n",
    "    pickle.dump(pt_inter_cgm, fp)\n",
    "\n",
    "# Save the dictionary with the actual blood glucose measurements    \n",
    "with open('pt_true_bgm.p', 'wb') as fp:\n",
    "    pickle.dump(pt_gold_data, fp)\n",
    "    \n",
    "# with open('pt_gwb_bgm.p', 'wb') as fp:\n",
    "#     pickle.dump(pt_inter_cgm, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create and test and training data set using pickle\n",
    "\n",
    "def make_test_train(data_set, percent_train):\n",
    "    length = percent_train*len(data.keys())\n",
    "    test = {}\n",
    "    train = {}\n",
    "    for i, (key, value) in enumerate(data_set.iteritems()):\n",
    "        if i < length:\n",
    "            train[key] = value\n",
    "        else:\n",
    "            test[key] = value\n",
    "    with open('train_data.p', 'wb') as fp:\n",
    "        pickle.dump(train, fp)\n",
    "    with open('test_data.p', 'wb') as fp:\n",
    "        pickle.dump(test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pt_cgm_bgm.p', 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "make_test_train(data, 0.7) # split the data into testing and training data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
